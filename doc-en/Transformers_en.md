** Transformers ** are a neural network architecture introduced by the article "Attention is all you Need" in 2017, which revolutionized the field of Natural Language Processing (NLP) and, more recently, computational vision and other areas of artificial intelligence.The central innovation of transformers is the ** attention mechanism ** (Attention), which allows the model to assign different levels of importance to the parts of an input sequence, such as words in a sentence, to better understand context and relationshipsamong them.

---

## ** Index on Transformers **

1. ** Introduction **
- Definition of Transformers
- Why are they important?

2. ** Basic architecture **
- Coder (Encoder)
- Decoder (Decoder)
- Self-ATTENTION (SELF)
- Embeddings and Positional Encoding

3. ** Attention mechanism operation **
- Attention Scale
- Attention masks
- Multi-HEAD Attention

4. ** Applications **
- Natural Language Processing (NLP)
- Text translation
- Text Summary
- Text generation
- Computational vision
- Transformers in images (Vision Transformers - VIT)
- Other areas
- Biology (protein structure)
- Recommendation Systems

5. ** Famous models based on transformers **
- Bert (bidirectional encoder representations from transformers)
- GPT (Generative Pre-Trained Transformer)
-T5 (Text-to-Text Transfer Transformer)
- Vit (Vision Transformer)

6. ** Advantages **
- Parallelism during training
- Better context capture in relation to RNNS and LSTMS models

7. ** Challenges **
- Need for great computational power
- Training cost

8. ** Implementations **
- Popular Frameworks (Hugging Face, Tensorflow, Pytorch)
- Basic examples of use

9. ** Future Perspectives **
- New Architectures derived
- Use in emerging areas

10. ** References and resources **
- Original articles and tutorials
- Library documentation

---