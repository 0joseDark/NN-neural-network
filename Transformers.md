**Transformers** são uma arquitetura de redes neurais introduzida pelo artigo "Attention is All You Need" em 2017, que revolucionaram o campo do processamento de linguagem natural (NLP) e, mais recentemente, visão computacional e outras áreas da inteligência artificial. A inovação central dos Transformers é o mecanismo de **atenção** (attention), que permite que o modelo atribua diferentes níveis de importância às partes de uma sequência de entrada, como palavras em uma frase, para melhor entender o contexto e as relações entre elas. 

---

## **Índice sobre Transformers**

1. **Introdução**
   - Definição de Transformers
   - Por que são importantes?

2. **Arquitetura Básica**
   - Codificador (Encoder)
   - Decodificador (Decoder)
   - Autoatenção (Self-Attention)
   - Embeddings e Positional Encoding

3. **Funcionamento do Mecanismo de Atenção**
   - Atenção Escalada (Scaled Dot-Product Attention)
   - Máscaras de Atenção
   - Multi-Head Attention

4. **Aplicações**
   - Processamento de Linguagem Natural (NLP)
     - Tradução de texto
     - Resumo de texto
     - Geração de texto
   - Visão Computacional
     - Transformers em imagens (Vision Transformers - ViT)
   - Outras áreas
     - Biologia (estrutura de proteínas)
     - Sistemas de recomendação

5. **Modelos Famosos Baseados em Transformers**
   - BERT (Bidirectional Encoder Representations from Transformers)
   - GPT (Generative Pre-trained Transformer)
   - T5 (Text-to-Text Transfer Transformer)
   - ViT (Vision Transformer)

6. **Vantagens**
   - Paralelismo durante o treinamento
   - Melhor captura de contexto em relação a modelos RNNs e LSTMs

7. **Desafios**
   - Necessidade de grande poder computacional
   - Custo de treinamento

8. **Implementações**
   - Frameworks populares (Hugging Face, TensorFlow, PyTorch)
   - Exemplos básicos de uso

9. **Perspectivas Futuras**
   - Novas arquiteturas derivadas
   - Uso em áreas emergentes

10. **Referências e Recursos**
    - Artigos originais e tutoriais
    - Documentação de bibliotecas

---
